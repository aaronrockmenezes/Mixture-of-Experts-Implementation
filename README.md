# Mixture-of-Experts-Implementation
This repo implements a Mixture of Experts layer and compares the performance of a BiLSTM model with and without the MoE layer on the CoNLL 2003 dataset.
